{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1mQlXBQUtLSk0QcOhx8UNchdl4RcvTqyt",
      "authorship_tag": "ABX9TyMc0wMyApvNFAK/xgAGATlf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lujain618/Deepfake_Detection_With_XAI/blob/main/DatasetLoading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_WIK2TOx0nM",
        "outputId": "6563f599-5549-40f0-b7d3-259e5df15ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libriries\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "6Az82dVsc_RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make Balanced_Dataset.csv folder with 890 videos from Celeb_synthesis, 590 Celeb_real and 300 Youtube_real"
      ],
      "metadata": {
        "id": "QfD3p2TVGbVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataset directory\n",
        "dataset_dir = \"/content/drive/MyDrive/DeepfakeDataset\"\n",
        "celeb_synthesis_dir = os.path.join(dataset_dir, \"Celeb-synthesis\")\n",
        "celeb_real_dir = os.path.join(dataset_dir, \"Celeb-real\")\n",
        "youtube_real_dir = os.path.join(dataset_dir, \"YouTube-real\")\n",
        "real_videos = [os.path.join(celeb_real_dir, video) for video in os.listdir(celeb_real_dir)]\n",
        "real_videos += [os.path.join(youtube_real_dir, video) for video in os.listdir(youtube_real_dir)]\n",
        "fake_videos = [os.path.join(celeb_synthesis_dir, video) for video in os.listdir(celeb_synthesis_dir)]\n",
        "fake_videos = random.sample(fake_videos, 890)  # randomly select 890 fake videos\n",
        "# create labels: 0 for real, 1 for fake\n",
        "real_labels = [0] * len(real_videos)\n",
        "fake_labels = [1] * len(fake_videos)\n",
        "# combine into a single dataset\n",
        "video_paths = real_videos + fake_videos\n",
        "labels = real_labels + fake_labels\n",
        "# create a Pandas DataFrame\n",
        "df = pd.DataFrame({'video_path': video_paths, 'label': labels})\n",
        "# save to CSV file\n",
        "csv_filename = '/content/drive/MyDrive/DeepfakeDataset/Balanced_dataset.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file '{csv_filename}' created successfully with {len(df)} entries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkRl2O9nleu2",
        "outputId": "f97f9cfc-6261-4c50-9663-0deb111f5fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file '/content/drive/MyDrive/DeepfakeDataset/Balanced_dataset.csv' created successfully with 1780 entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model selcetion: split the dataset to 70% train, 20% val and 10% test"
      ],
      "metadata": {
        "id": "Ph5pOCNxsmGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "csv_path = \"Balanced_dataset.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Split into training (70%) and temp set (30%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "\n",
        "# Further split temp set into validation (20% of total) and test (10% of total)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set: {len(train_df)} samples\")\n",
        "print(f\"Validation set: {len(val_df)} samples\")\n",
        "print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "# Save the splits\n",
        "train_df.to_csv(\"/content/drive/MyDrive/DeepfakeDataset/train.csv\", index=False)\n",
        "val_df.to_csv(\"/content/drive/MyDrive/DeepfakeDataset/val.csv\", index=False)\n",
        "test_df.to_csv(\"/content/drive/MyDrive/DeepfakeDataset/test.csv\", index=False)\n",
        "\n",
        "print(\"Splits saved as train.csv, val.csv, and test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky2807Y83MDN",
        "outputId": "e1532080-0cee-4c87-ae58-d051b90d12c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 1246 samples\n",
            "Validation set: 356 samples\n",
            "Test set: 178 samples\n",
            "Splits saved as train.csv, val.csv, and test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting 30 frames from each video and resizing them to 112x112"
      ],
      "metadata": {
        "id": "E7PefcxM4HSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train.csv\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DeepfakeDataset/train.csv\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"/content/drive/MyDrive/DeepfakeDataset/train_frames\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def extract_frames(video_path, save_path, num_frames=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    count = 0\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened() and count < num_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize frame to 112x112\n",
        "        frame = cv2.resize(frame, (112, 112))\n",
        "\n",
        "        # Save frame\n",
        "        frame_filename = os.path.join(save_path, f\"frame_{count:02d}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# Process each video\n",
        "for index, row in df.iterrows():\n",
        "    video_path = row[\"video_path\"]\n",
        "    label = str(row[\"label\"])  # Convert label to string for folder naming\n",
        "\n",
        "    # Create label folder\n",
        "    label_folder = os.path.join(output_dir, label)\n",
        "    os.makedirs(label_folder, exist_ok=True)\n",
        "\n",
        "    # Create video folder inside label folder\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    video_folder = os.path.join(label_folder, video_name)\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    # Extract and save frames\n",
        "    extract_frames(video_path, video_folder)\n",
        "\n",
        "print(\"Frame extraction complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n2knu0K7d28",
        "outputId": "e41daf5c-5d65-4ce6-9e3b-b13c0097cbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply min-max normalization and one hot encoding"
      ],
      "metadata": {
        "id": "xLQx4qysFARz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories\n",
        "input_dir = \"/content/drive/MyDrive/DeepfakeDataset/train_frames\"\n",
        "output_dir = \"/content/drive/MyDrive/DeepfakeDataset/train_processed\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def minmax_normalize(image):\n",
        "    \"\"\"Normalize pixel values to range [0, 1]\"\"\"\n",
        "    return image.astype(np.float32) / 255.0\n",
        "\n",
        "def one_hot_encode(image):\n",
        "    \"\"\"One-hot encode an image (for RGB images, keep channels unchanged)\"\"\"\n",
        "    if len(image.shape) == 2:  # Grayscale image\n",
        "        return np.eye(256)[image]  # One-hot encoding for grayscale (if needed)\n",
        "    else:\n",
        "        return image  # RGB images already have 3 channels, so return as is\n",
        "\n",
        "# Process all frames\n",
        "for label in os.listdir(input_dir):  # 0 or 1\n",
        "    label_folder = os.path.join(input_dir, label)\n",
        "    processed_label_folder = os.path.join(output_dir, label)\n",
        "    os.makedirs(processed_label_folder, exist_ok=True)\n",
        "\n",
        "    for video_name in os.listdir(label_folder):\n",
        "        video_folder = os.path.join(label_folder, video_name)\n",
        "        processed_video_folder = os.path.join(processed_label_folder, video_name)\n",
        "        os.makedirs(processed_video_folder, exist_ok=True)\n",
        "\n",
        "        for frame_name in os.listdir(video_folder):\n",
        "            frame_path = os.path.join(video_folder, frame_name)\n",
        "            frame = cv2.imread(frame_path)  # Read frame\n",
        "\n",
        "            # Normalize and One-hot encode\n",
        "            frame = minmax_normalize(frame)\n",
        "            frame = one_hot_encode(frame)\n",
        "\n",
        "            # Save processed frame\n",
        "            processed_frame_path = os.path.join(processed_video_folder, frame_name)\n",
        "            np.save(processed_frame_path.replace(\".jpg\", \".npy\"), frame)  # Save as .npy\n",
        "\n",
        "print(\"Processing complete! Frames are saved in 'train_processed'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gccteT0FbVY",
        "outputId": "0dcbd2b4-489a-49d6-8ee8-b9f4df9d5171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete! Frames are saved in 'train_processed'.\n"
          ]
        }
      ]
    }
  ]
}